import sys
import argparse
import json
import logging

import numpy as np
import torch
import torch.nn as nn
from torch import nn, optim, cuda
from torch.utils.data import TensorDataset, DataLoader
import torch.nn.functional as F

from nlplingo.tasks.event_domain import EventDomain
from nlplingo.nn.extractor import Extractor
from nlplingo.annotation.ingestion import prepare_docs
from nlplingo.tasks.eventtrigger.generator import EventTriggerExampleGenerator
from nlplingo.tasks.eventtrigger.run import generate_trigger_data_feature
from nlplingo.embeddings.word_embeddings import load_embeddings
from nlplingo.common.scoring import evaluate_f1
from nlplingo.common.scoring import print_score_breakdown
from nlplingo.common.scoring import write_score_to_file
from nlplingo.tasks.eventtrigger.run import get_predicted_positive_triggers

logger = logging.getLogger(__name__)


class Net(nn.Module):
    def __init__(self, input_size, output_size):
        super(Net, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.l1 = nn.Linear(input_size, 768)
        self.l2 = nn.Linear(768, 768)
        self.l3 = nn.Linear(768, output_size)

    def forward(self, x):
        """
        We need to flatten before we give `x` to the fully connected layer. So we tell PyTorch to reshape the tensor.
        `x.view(-1, self.input_size)` tells PyTorch to use `input_size` as the number of columns,
        but decide the number of rows by itself.

        NOTE: `view` shares the underlying data with the original tensor, so it is really a view into the old tensor
        instead of creating a brand new one

        We return logits, i.e. we did not apply activation
        """
        x = x.view(-1, self.input_size)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        return self.l3(x)   # return logits


device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')


def train_trigger_multilayer(params, word_embeddings, trigger_extractor):
    """
    :type params: dict
    :type word_embeddings: dict[str:nlplingo.embeddings.word_embeddings.WordEmbedding]
    :type trigger_extractor: nlplingo.nn.extractor.Extractor

    #### data_list
    This is generated by for example the `generate_trigger_data_feature()` method.

    data_list.shape = (#features, #examples, #dimensions-for-this-feature)

    For instance, if you have the following piece of code where each example has 2 features: 'trigger' and 'arg':
    ```
    data = defaultdict(list)
    x1_trigger = [0.1, 0.2, 0.22]
    x2_trigger = [0.3, 0.4, 0.44]
    x1_arg = [0.5, 0.6, 0.66]
    x2_arg = [0.7, 0.8, 0.88]
    data['trigger'].append(x1_trigger)
    data['trigger'].append(x2_trigger)
    data['arg'].append(x1_arg)
    data['arg'].append(x2_arg)

    data_list = [np.asarray(data[k]) for k in data]
    ```

    Then doing `torch.from_numpy(np.array(data_list)).shape` gives `torch.Size([2, 2, 3])`

    #### train_label
    This is of shape (#examples, #labels)
    So e.g. when you do `torch.from_numpy(np.array(train_label))`, and you get:
    ```
    tensor([[-2.0,  0.1],
            [ 0.7,  0.4],
            [-1.5, -1.6]])
    ```
    The above is for #examples=3 and #labels=2.
    Then when you do `torch.from_numpy(np.array(train_label)).max(1)`, you get a tuple:
    ```
    torch.return_types.max(
    values=tensor([ 0.1, 0.7, -1.5]),
    indices=tensor([1, 0, 0]))
    ```
    And we can do `torch.from_numpy(np.array(train_label)).max(1)[1]` to get the indices `tensor([1, 0, 0])`
    """

    feature_generator = trigger_extractor.feature_generator
    """:type: nlplingo.event.trigger.feature.EventTriggerFeatureGenerator"""
    example_generator = trigger_extractor.example_generator
    """:type: nlplingo.event.trigger.generator.EventTriggerExampleGenerator"""
    trigger_model = trigger_extractor.extraction_model
    """:type: nlplingo.nn.trigger_model.TriggerModel"""

    # logger.debug('type(feature_generator)={}'.format(type(feature_generator)))
    # logger.debug('type(example_generator)={}'.format(type(example_generator)))
    # logger.debug('type(trigger_model)={}'.format(type(trigger_model)))

    # prepare dataset for sample generation
    logger.info("Preparing docs")
    train_docs = prepare_docs(params['data']['train']['filelist'], word_embeddings, params)
    dev_docs = prepare_docs(params['data']['dev']['filelist'], word_embeddings, params)
    logger.info("Applying domain")
    for doc in train_docs + dev_docs:
        doc.apply_domain(trigger_extractor.domain)

    (train_examples, train_data, train_data_list, train_label) = (
        generate_trigger_data_feature(example_generator, train_docs, feature_generator))

    (dev_examples, dev_data, dev_data_list, dev_label) = (
        generate_trigger_data_feature(example_generator, dev_docs, feature_generator))

    """
    squeeze() returns a tensor with all dimensions of size 1 removed
    Since `train_data_list` shape is: (#features, #examples, #dimensions-in-this-feature). 
    If we are just using a single feature (trigger window), 
    then after `squeeze()` we will be left with (#examples, #dimensions-in-this-feature), which is what is needed.

    CAUTION: if you are using 2 features, e.g. (trigger_window, argument_window), then you need to further manipulate 
    `train_data_list` to be a 2-dimensional matrix of : (#examples, #features X #feature-dimensions)
    """
    train_data = TensorDataset(torch.from_numpy(np.array(train_data_list)).squeeze(),
                               torch.from_numpy(np.array(train_label)).max(1)[1])
    dev_data = TensorDataset(torch.from_numpy(np.array(dev_data_list)).squeeze(),
                             torch.from_numpy(np.array(dev_label)).max(1)[1])

    input_size = len(np.array(train_data_list).squeeze()[0])  # number of input features in each example, e.g. 3072
    output_size = len(dev_label[0])  # number of output labels/classes, e.g. 33
    print('input_size=%d output_size=%d' % (input_size, output_size))

    train_loader = DataLoader(train_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=True)
    dev_loader = DataLoader(dev_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=False)

    # ========= training
    model = Net(input_size, output_size)
    model.to(device)
    criterion = nn.CrossEntropyLoss()  # this does softmax on the target class, then negative-log
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

    for epoch in range(trigger_extractor.hyper_parameters.epoch):
        model.train()
        for batch_index, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)

            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            if (batch_index % 100) == 0:
                N = len(train_loader.dataset)
                N_seen = batch_index * len(data)
                batch_proportion = 100. * batch_index / len(train_loader)
                print('Train epoch {} | Batch status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(epoch, N_seen, N,
                                                                                             batch_proportion,
                                                                                             loss.item()))

    # saving model file
    torch.save(model.state_dict(), trigger_extractor.model_file)

    # model.load_state_dict(torch.load(trigger_extractor.model_file))

    predicted_positive_dev_triggers = evaluate(model, dev_loader, criterion, dev_examples, dev_label, trigger_extractor,
                                               params['train.score_file'])

    # if 'test' in params['data']:
    #     test_docs = prepare_docs(params['data']['test']['filelist'], word_embeddings, params)
    #
    #     for doc in test_docs:
    #         doc.apply_domain(trigger_extractor.domain)
    #
    #     # Generate data
    #     (test_examples, test_data, test_data_list, test_label) = (
    #         generate_trigger_data_feature(trigger_extractor.example_generator, test_docs, trigger_extractor.feature_generator))
    #
    #     test_data = TensorDataset(torch.from_numpy(np.array(test_data_list)).squeeze(),
    #                              torch.from_numpy(np.array(test_label)).max(1)[1])
    #     test_loader = DataLoader(test_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=False)
    #
    #     predicted_positive_test_triggers = evaluate_lstm(model, test_loader, criterion, test_examples, test_label, trigger_extractor, params['test.score_file'])


def test_trigger_multilayer(params, word_embeddings, trigger_extractor):
    """
    :type params: dict
    :type word_embeddings: nlplingo.embeddings.WordEmbedding
    :type trigger_extractor: nlplingo.nn.extractor.Extractor
    """

    test_docs = prepare_docs(params['data']['test']['filelist'], word_embeddings, params)

    for doc in test_docs:
        doc.apply_domain(trigger_extractor.domain)

    feature_generator = trigger_extractor.feature_generator
    """:type: nlplingo.event.trigger.feature.EventTriggerFeatureGenerator"""
    example_generator = trigger_extractor.example_generator
    """:type: nlplingo.event.trigger.generator.EventTriggerExampleGenerator"""
    trigger_model = trigger_extractor.extraction_model
    """:type: nlplingo.nn.trigger_model.TriggerModel"""

    # Generate data
    (test_examples, test_data, test_data_list, test_label) = (generate_trigger_data_feature(example_generator, test_docs, feature_generator))

    test_data = TensorDataset(torch.from_numpy(np.array(test_data_list)).squeeze(), torch.from_numpy(np.array(test_label)).max(1)[1])
    test_loader = DataLoader(test_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=False)

    input_size = len(np.array(test_data_list).squeeze()[0])  # number of input features in each example, e.g. 3072
    output_size = len(test_label[0])  # number of output labels/classes, e.g. 33
    print('input_size=%d output_size=%d' % (input_size, output_size))

    model = Net(input_size, output_size)
    model.to(device)
    model.load_state_dict(torch.load(trigger_extractor.model_file))

    criterion = nn.CrossEntropyLoss()  # this does softmax on the target class, then negative-log

    predicted_positive_test_triggers = evaluate(model, test_loader, criterion, test_examples, test_label, trigger_extractor,
                                               params['test.score_file'])



class LstmNet(nn.Module):
    def __init__(self, output_size, input_size, hidden_dim, n_layers, seq_len, drop_prob=0.5, bidirectional=False):
        super(LstmNet, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        self.bidirectional = bidirectional

        #self.embedding = nn.Embedding(vocab_size, embedding_dim)
        #                    input_size    hidden_size
        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=False)
        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=self.bidirectional)
        self.dropout = nn.Dropout(0.2)

        if self.bidirectional:
            self.fc1 = nn.Linear(hidden_dim*2, hidden_dim*2)
        else:
            self.fc1 = nn.Linear(hidden_dim, hidden_dim)
        self.init_weights(self.fc1)

        if self.bidirectional:
            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)
        else:
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.init_weights(self.fc2)

        self.fc3 = nn.Linear(hidden_dim, output_size)
        self.init_weights(self.fc3)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x, h):
        # x.shape     # => torch.Size([100, 50, 3072]) (batch-size, seq-len, input-size)
        # h[0].shape  # => torch.Size([1, 100, 768])   (num-layers, batch-size, hidden-size)
        # h[1].shape  # => torch.Size([1, 100, 768])   (num-layers, batch-size, hidden-size)

        batch_size = x.size(0)

        # you can also do the following
        # ```
        # h0 = torch.zeros(self.n_layers * 1, x.size(0), self.hidden_dim).to(device)  # 2 for bidirection
        # c0 = torch.zeros(self.n_layers * 1, x.size(0), self.hidden_dim).to(device)
        # lstm_out, hidden = self.lstm(x, (h0, c0))
        # ```
        lstm_out, hidden = self.lstm(x, h)
        """
        lstm_out.shape  # => torch.Size([100, 50, 768]) , (batch-size, seq-len, hidden-size)
        len(hidden)     # => 2
        hidden[0].shape # => torch.Size([1, 100, 768]) , final hidden state
        hidden[1].shape # => torch.Size([1, 100, 768]) , final cell state

        NOTE: lstm_out[5][-1] == hidden[0][0][5]
        What this means is that lstm_out gives the hidden-state of (all examples in batch, every time sequence)
        And lstm_out[5][-1] gets the LAST hidden-state of the 6th example in the batch 
        """

        if self.bidirectional:
            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim*2)
        else:
            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)

        fc1_out = F.relu(self.fc1(lstm_out))
        fc2_out = F.relu(self.fc2(fc1_out))
        d_out = self.dropout(fc2_out)      # d_out.shape = torch.Size([5000, 768])
        out = self.fc3(d_out)              # logits , out.shape = torch.Size([5000, 34])

        out = out.view(x.size(0), self.seq_len, -1)     # out.shape = torch.Size([100, 50, 34])
        return out, hidden


    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        # next(self.parameters()).data.new() : grabbing the first parameter in the model and making a new tensor of the same type with specified dimensions.

        n_layers = self.n_layers	# you have to double this, if using biLSTM
        #n_layers = 2
        if self.bidirectional:
            hidden = (weight.new(n_layers*2, batch_size, self.hidden_dim).zero_().to(device),
                      weight.new(n_layers*2, batch_size, self.hidden_dim).zero_().to(device))
        else:
            hidden = (weight.new(n_layers, batch_size, self.hidden_dim).zero_().to(device),
                      weight.new(n_layers, batch_size, self.hidden_dim).zero_().to(device))
        return hidden

    def init_weights(self, layer):
        if type(layer) == nn.Linear:
            print("Initiliaze layer with nn.init.xavier_uniform_: {}".format(layer))
            torch.nn.init.xavier_uniform_(layer.weight)
            layer.bias.data.fill_(0.01)


def predict_lstm(model, criterion, data_loader, examples, batch_size):
    model.eval()
    loss = 0
    predictions = None
    #h = model.init_hidden(batch_size)
    with torch.no_grad():
        for batch_index, (data, target) in enumerate(data_loader):
            #h = tuple([e.data[:, 0:data.size(0), :] for e in h])
            h = model.init_hidden(data.size(0))
            data, target = data.to(device), target.to(device)
            output, h = model(data, h)

            output_head = None
            for i in range(output.shape[0]):
                eg = examples[batch_index * batch_size + i]
                token_index = eg.anchor.head().index_in_sentence

                if output_head is None:
                    output_head = torch.unsqueeze(output[i][token_index], dim=0)
                else:
                    output_head = torch.cat((output_head, torch.unsqueeze(output[i][token_index], dim=0)))

            loss += criterion(output_head, target)

            # print('type(output_head)=', type(output_head))              # torch.Tensor
            # print('output_head.shape=', output_head.shape)              # torch.Size([100, 34])
            # print('type(output_head.data)=', type(output_head.data))    # torch.Tensor
            # print('output_head.data.shape=', output_head.data.shape)    # torch.Size([100, 34])
            # pred = output_head.data.max(1, keepdim=True)[1]             # get index of max

            if predictions is None:
                predictions = output_head.data.cpu().numpy()
            else:
                predictions = np.vstack((predictions, output_head.data.cpu().numpy()))

                # correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    loss /= len(data_loader.dataset)
    print(f'=====\nAverage loss: {loss:.4f}')

    return predictions

def train_trigger_lstm(params, word_embeddings, trigger_extractor, bidirectional=False):
    """
    :type params: dict
    :type word_embeddings: dict[str:nlplingo.embeddings.word_embeddings.WordEmbedding]
    :type trigger_extractor: nlplingo.nn.extractor.Extractor
    """

    # prepare dataset for sample generation
    logger.info("Preparing docs")
    train_docs = prepare_docs(params['data']['train']['filelist'], word_embeddings, params)
    dev_docs = prepare_docs(params['data']['dev']['filelist'], word_embeddings, params)
    logger.info("Applying domain")
    for doc in train_docs + dev_docs:
        doc.apply_domain(trigger_extractor.domain)

    batch_size = trigger_extractor.hyper_parameters.batch_size

    input_size = 3072

    (train_examples, train_data, train_data_list, train_label) = (generate_trigger_data_feature(trigger_extractor.example_generator, train_docs, trigger_extractor.feature_generator))
    #train_size = int(len(train_examples) / batch_size) * batch_size
    #train_examples = train_examples[0:train_size]
    #for i in range(len(train_data_list)):
    #    train_data_list[i] = train_data_list[i][:, :, 0:input_size]
        #train_data_list[i] = train_data_list[i][0:train_size]
        #print(train_data_list[i].shape)  # [N, seq_len, input_size/num#_features]
    #train_label = train_label[0:train_size]
    # train_data_list[0].shape= (4300, 200, 3072)
    print('type(train_label)=', type(train_label))	# np.ndarray
    print('train_label.shape=', train_label.shape)	# (4200, 34)
    print('train_label=', train_label)
    train_labels = []
    for i in range(len(train_label)):
        train_labels.append([np.argmax(train_label[i]), train_examples[i].anchor.head().index_in_sentence])
        #train_labels.append((train_label[i], train_examples[i].anchor.head().index_in_sentence))
    train_labels = np.array(train_labels)
    print('type(train_labels)=', type(train_labels))	# np.ndarray
    print('train_labels.shape=', train_labels.shape)	# (4200, 2)
    print('train_labels=', train_labels)

    #for i in range(100):
    #    token_index = int(train_labels[i][1])
    #    print('train_labels[', i, ']=', train_labels[i], 'embeddings=', train_data_list[0][i][token_index][0], train_data_list[0][i][token_index][-1], ' trigger=', train_examples[i].anchor.head().text)

    #a = torch.from_numpy(np.array(train_label)).max(1)[1]
    #print(a)		# tensor([33, 33, 33, ..., 33, 33, 33])
    #print(type(a))	# torch.Tensor
    #print(a.shape)	# torch.Size([4200])

    (dev_examples, dev_data, dev_data_list, dev_label) = (generate_trigger_data_feature(trigger_extractor.example_generator, dev_docs, trigger_extractor.feature_generator))
    #dev_size = int(len(dev_examples) / batch_size) * batch_size
    #dev_examples = dev_examples[0:dev_size]
    #for i in range(len(dev_data_list)):
    #    dev_data_list[i] = dev_data_list[i][:, :, 0:input_size]
        #dev_data_list[i] = dev_data_list[i][0:dev_size]
    #dev_label = dev_label[0:dev_size]


    #train_data = TensorDataset(torch.from_numpy(np.array(train_data_list)).squeeze(), torch.from_numpy(np.array(train_label)).max(1)[1])
    train_data = TensorDataset(torch.from_numpy(np.array(train_data_list)).squeeze(), torch.from_numpy(train_labels))
    dev_data = TensorDataset(torch.from_numpy(np.array(dev_data_list)).squeeze(), torch.from_numpy(np.array(dev_label)).max(1)[1])

    #input_size = len(np.array(train_data_list).squeeze()[0])
    output_size = len(train_label[0])
    #output_size = len(dev_label[0])
    #print('input_size=%d output_size=%d' % (input_size, output_size))

    train_loader = DataLoader(train_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=True)
    dev_loader = DataLoader(dev_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=False)


    # ========= training
    model = LstmNet(output_size, input_size, 512, 1, 50, bidirectional=bidirectional)
    model.to(device)
    criterion = nn.CrossEntropyLoss()  # this is logSoftmax + nll_loss
    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)

    model.train()

    #h = None
    for epoch in range(trigger_extractor.hyper_parameters.epoch):
        #h = model.init_hidden(batch_size)
        for batch_index, (data, target) in enumerate(train_loader):
            #print(data.size())	# [100, 50, 3072], i.e. (batch-size, seq-len, feature-input-size)

            trigger_token_indices = target.data[:, 1].numpy()
            labels = target.data[:, 0]

            # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history
            #h = tuple([e.data[:, 0:data.size(0), :] for e in h])
            h = model.init_hidden(data.size(0))
            # h[0].shape = torch.Size([1, 100, 768])    (#layers, batch-size, hidden-dimension)

            data, labels = data.to(device), labels.to(device)
            optimizer.zero_grad()

            output, h = model(data, h)

            output_head = None
            for i in range(output.shape[0]):        # for each example in batch
                token_index = trigger_token_indices[i]
                if output_head is None:
                    output_head = torch.unsqueeze(output[i][token_index], dim=0)
                else:
                    output_head = torch.cat((output_head, torch.unsqueeze(output[i][token_index], dim=0)))
            # output_head.shape = torch.Size([100, 34]), i.e. (batch-size, #classes)

            loss = criterion(output_head, labels)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 5)     # TODO this should be a parameter
            optimizer.step()

            if (batch_index % 10) == 0:
                N = len(train_loader.dataset)
                N_seen = batch_index * len(data)
                batch_proportion = 100. * batch_index / len(train_loader)
                print('Train epoch {}/{} | Batch status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(epoch, trigger_extractor.hyper_parameters.epoch, N_seen, N, batch_proportion, loss.item()))

    # saving model file
    torch.save(model.state_dict(), trigger_extractor.model_file)

    predictions = predict_lstm(model, criterion, dev_loader, dev_examples, batch_size)
    # model.eval()
    # loss = 0
    # predictions = None
    # h = model.init_hidden(batch_size)
    # with torch.no_grad():
    #     for batch_index, (data, target) in enumerate(dev_loader):
    #         h = tuple([e.data[:, 0:data.size(0), :] for e in h])
    #         data, target = data.to(device), target.to(device)
    #         output, h = model(data, h)
    #
    #         output_head = None
    #         for i in range(output.shape[0]):
    #             eg = dev_examples[batch_index * batch_size + i]
    #             token_index = eg.anchor.head().index_in_sentence
    #
    #             if output_head is None:
    #                 output_head = torch.unsqueeze(output[i][token_index], dim=0)
    #             else:
    #                 output_head = torch.cat((output_head, torch.unsqueeze(output[i][token_index], dim=0)))
    #
    #         loss += criterion(output_head, target)
    #
    #         #print('type(output_head)=', type(output_head))              # torch.Tensor
    #         #print('output_head.shape=', output_head.shape)              # torch.Size([100, 34])
    #         #print('type(output_head.data)=', type(output_head.data))    # torch.Tensor
    #         #print('output_head.data.shape=', output_head.data.shape)    # torch.Size([100, 34])
    #         #pred = output_head.data.max(1, keepdim=True)[1]             # get index of max
    #
    #         if predictions is None:
    #             predictions = output_head.data.cpu().numpy()
    #         else:
    #             predictions = np.vstack((predictions, output_head.data.cpu().numpy()))
    #
    #         #correct += pred.eq(target.data.view_as(pred)).cpu().sum()
    #
    # loss /= len(dev_loader.dataset)
    # print(f'=====\nAverage loss: {loss:.4f}')

    score, score_breakdown = evaluate_f1(predictions, dev_label, extractor.domain.get_event_type_index('None'))
    logger.info(score.to_string())
    print_score_breakdown(extractor, score_breakdown)
    write_score_to_file(extractor, score, score_breakdown, params['train.score_file'])



    #predicted_positive_dev_triggers = evaluate(model, dev_loader, criterion, dev_examples, dev_label, trigger_extractor, params['train.score_file'])


def test_trigger_lstm(params, word_embeddings, trigger_extractor, bidirectional=False):
    """
    :type params: dict
    :type word_embeddings: nlplingo.embeddings.WordEmbedding
    :type trigger_extractor: nlplingo.nn.extractor.Extractor
    """

    test_docs = prepare_docs(params['data']['test']['filelist'], word_embeddings, params)

    for doc in test_docs:
        doc.apply_domain(trigger_extractor.domain)

    feature_generator = trigger_extractor.feature_generator
    """:type: nlplingo.event.trigger.feature.EventTriggerFeatureGenerator"""
    example_generator = trigger_extractor.example_generator
    """:type: nlplingo.event.trigger.generator.EventTriggerExampleGenerator"""
    trigger_model = trigger_extractor.extraction_model
    """:type: nlplingo.nn.trigger_model.TriggerModel"""

    # Generate data
    (test_examples, test_data, test_data_list, test_label) = (generate_trigger_data_feature(example_generator, test_docs, feature_generator))

    test_data = TensorDataset(torch.from_numpy(np.array(test_data_list)).squeeze(), torch.from_numpy(np.array(test_label)).max(1)[1])
    test_loader = DataLoader(test_data, batch_size=trigger_extractor.hyper_parameters.batch_size, shuffle=False)

    input_size = 3072
    #print(np.array(test_data_list).squeeze().shape) # (4513, 50, 3072) , (#examples, seq-len, input-size)
    output_size = len(test_label[0])  # number of output labels/classes, e.g. 33
    print('input_size=%d output_size=%d' % (input_size, output_size))

    model = LstmNet(output_size, input_size, 512, 1, 50, bidirectional=bidirectional)
    model.to(device)
    model.load_state_dict(torch.load(trigger_extractor.model_file))

    criterion = nn.CrossEntropyLoss()  # this does softmax on the target class, then negative-log

    batch_size = trigger_extractor.hyper_parameters.batch_size
    predictions = predict_lstm(model, criterion, test_loader, test_examples, batch_size)

    score, score_breakdown = evaluate_f1(predictions, test_label, extractor.domain.get_event_type_index('None'))
    logger.info(score.to_string())
    print_score_breakdown(extractor, score_breakdown)
    write_score_to_file(extractor, score, score_breakdown, params['test.score_file'])



def evaluate(model, data_loader, criterion, examples, labels, extractor, score_file):
    model.eval()
    loss = 0
    correct = 0
    predictions = None
    for data, target in data_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss += criterion(output, target).item()

        # keepdim=True makes output tensors of the same size as `input`,
        # except in the dimension `dim` where they are of size 1
        # In this case, output.data is a matrix of predicted probabilities (#examples, #labels)
        pred_probs = output.data.max(1, keepdim=True)[0]  # get max probabilities
        pred = output.data.max(1, keepdim=True)[1]  # get max indices
        #print('type(output.data)', type(output.data))
        #print('output.data.shape=', output.data.shape)
        if predictions is None:
            predictions = output.data.cpu().numpy()
        else:
            predictions = np.vstack((predictions, output.data.cpu().numpy()))

        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    loss /= len(data_loader.dataset)
    print(f'=====\nAverage loss: {loss:.4f}')

    score, score_breakdown = evaluate_f1(predictions, labels, extractor.domain.get_event_type_index('None'))
    logger.info(score.to_string())
    print_score_breakdown(extractor, score_breakdown)
    write_score_to_file(extractor, score, score_breakdown, score_file)

    return get_predicted_positive_triggers(predictions, examples, extractor)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', required=True)  # train_trigger, train_arg, test_trigger, test_arg
    parser.add_argument('--params', required=True)
    args = parser.parse_args()

    with open(args.params) as f:
        params = json.load(f)
    print(json.dumps(params, sort_keys=True, indent=4))

    # ==== loading of embeddings ====
    embeddings = load_embeddings(params)

    load_extractor_models_from_file = False
    #if args.mode in {'test_trigger', 'test_argument', 'decode_trigger_argument', 'decode_trigger'}:
    #    load_extractor_models_from_file = True

    trigger_extractors = []
    argument_extractors = []
    """:type: list[nlplingo.model.extractor.Extractor]"""
    for extractor_params in params['extractors']:
        extractor = Extractor(params, extractor_params, embeddings, load_extractor_models_from_file)
        if extractor.model_type.startswith('event-trigger_'):
            trigger_extractors.append(extractor)
        elif extractor.model_type.startswith('event-argument_'):
            argument_extractors.append(extractor)
        else:
            raise RuntimeError('Extractor model type: {} not implemented.'.format(extractor.model_type))

    #if 'domain_ontology.scoring' in params:
    #    scoring_domain = EventDomain.read_domain_ontology_file(params.get_string('domain_ontology.scoring'), 'scoring')
    #else:
    #    scoring_domain = None

    if args.mode == 'train_trigger_multilayer':
        train_trigger_multilayer(params, embeddings, trigger_extractors[0])
    elif args.mode == 'test_trigger_multilayer':
        test_trigger_multilayer(params, embeddings, trigger_extractors[0])
    elif args.mode == 'train_trigger_lstm':
        train_trigger_lstm(params, embeddings, trigger_extractors[0], bidirectional=False)
    elif args.mode == 'test_trigger_lstm':
        test_trigger_lstm(params, embeddings, trigger_extractors[0], bidirectional=False)
    elif args.mode == 'train_trigger_bilstm':
        train_trigger_lstm(params, embeddings, trigger_extractors[0], bidirectional=True)
    elif args.mode == 'test_trigger_bilstm':
        test_trigger_lstm(params, embeddings, trigger_extractors[0], bidirectional=True)
    else:
        raise RuntimeError('mode: {} is not implemented!'.format(args.mode))
